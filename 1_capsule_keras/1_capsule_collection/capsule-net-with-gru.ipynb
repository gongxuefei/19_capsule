{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# print(os.listdir(\"./input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './input/dialogue_vectors.txt'\n",
    "# #train = pd.read_csv('../input/cleaned-toxic-comments/train_preprocessed.csv')\n",
    "train= pd.read_table('./input/train1600.txt',names=['text','class'])\n",
    "# #test = pd.read_csv('../input/cleaned-toxic-comments/test_preprocessed.csv')\n",
    "test = pd.read_table('./input/test1600.txt',names=['text','class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def del_label(doc):\n",
    "    return re.findall(\"__label__(.*)\",doc)[0]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['class']=train['class'].apply(del_label)\n",
    "test['class']=test['class'].apply(del_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "_cell_guid": "e8bd3575-f711-4ca6-a653-8ec1c74c0204",
    "_uuid": "cf43ac37cbd14d8baa088648c2275123550135d6"
   },
   "outputs": [],
   "source": [
    "train[\"text\"].fillna(\"fillna\")\n",
    "test[\"text\"].fillna(\"fillna\")\n",
    "X_train = train[\"text\"]\n",
    "Y_train = train[\"class\"]\n",
    "X_test = test[\"text\"]\n",
    "Y_test=test[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "_cell_guid": "da409613-3688-4d2e-a072-f67dee02617b",
    "_uuid": "efad6a0ecd758a759f14287a69bfd9cafa8c8fb2"
   },
   "outputs": [],
   "source": [
    "max_features=1000\n",
    "maxlen=15\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "_cell_guid": "3b0804b5-d64e-474e-8252-78daa4a4be62",
    "_uuid": "70c576f3b0afd3e779df184f788107fb51233424"
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "_cell_guid": "7a665c08-b4a9-4792-b40b-481b3da907e5",
    "_uuid": "b07e998ccedaf3aaaf4b4e67b207ad5490eb24f7"
   },
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "XX_train=tok.texts_to_sequences(X_train)\n",
    "XX_test=tok.texts_to_sequences(X_test)\n",
    "x_train=sequence.pad_sequences(XX_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(XX_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "tok_y=text.Tokenizer(num_words=22)\n",
    "tok_y.fit_on_texts(list(Y_train)+list(Y_test))\n",
    "\n",
    "YY_train=tok_y.texts_to_sequences(Y_train)\n",
    "YY_train=np.reshape(np.array(YY_train),np.array(YY_train).shape[0])-1\n",
    "\n",
    "YY_test=tok_y.texts_to_sequences(Y_test)\n",
    "YY_test=np.reshape(np.array(YY_test),np.array(YY_test).shape[0])-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class=YY_train.max()-YY_train.min()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = utils.to_categorical(YY_train, num_class)\n",
    "y_test = utils.to_categorical(YY_test, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "_cell_guid": "9e57a7cb-c061-4361-bbe2-05c0486a3f18",
    "_uuid": "9488bc9d68dfd1fde1f99d23a9f1ed7b30ceb87f"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38917"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "_cell_guid": "e2490100-fc9c-4e46-ae84-7dfa65fcddba",
    "_uuid": "d56ad119931a971b2588355deb726a045764c9ad"
   },
   "outputs": [],
   "source": [
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 300)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\n",
    "gru_len = 100\n",
    "Routings = 3\n",
    "Num_capsule = 21\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "_cell_guid": "1a4a1cf3-7faf-4ee4-a72e-a258169778a5",
    "_uuid": "560d3faac051bbb95dae6f1bf7013d52b404533c"
   },
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(max_features,\n",
    "                            embed_size,\n",
    "                            input_length=maxlen,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(21, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 15, 300)           300000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 15, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 15, 200)           240600    \n",
      "_________________________________________________________________\n",
      "capsule_5 (Capsule)          (None, 21, 16)            67200     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 21)                7077      \n",
      "=================================================================\n",
      "Total params: 614,877\n",
      "Trainable params: 314,877\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "_cell_guid": "46df26aa-adcd-4b2c-8644-76a1e51df2bc",
    "_uuid": "19975febdf6a0bd3077d8a92da13bb433085ce80",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/algor/gongxf/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 5\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.7, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1049, 15), (1049, 21), (1499, 15))"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tra.shape,y_tra.shape,x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 661us/step - loss: 0.0175 - acc: 0.9951 - val_loss: 0.0367 - val_acc: 0.9885\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 657us/step - loss: 0.0173 - acc: 0.9949 - val_loss: 0.0360 - val_acc: 0.9875\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 655us/step - loss: 0.0168 - acc: 0.9954 - val_loss: 0.0366 - val_acc: 0.9880\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 658us/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0349 - val_acc: 0.9888\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 661us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0353 - val_acc: 0.9877\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 665us/step - loss: 0.0156 - acc: 0.9953 - val_loss: 0.0364 - val_acc: 0.9874\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 662us/step - loss: 0.0155 - acc: 0.9958 - val_loss: 0.0379 - val_acc: 0.9874\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 663us/step - loss: 0.0155 - acc: 0.9953 - val_loss: 0.0374 - val_acc: 0.9874\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 667us/step - loss: 0.0149 - acc: 0.9959 - val_loss: 0.0390 - val_acc: 0.9875\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 657us/step - loss: 0.0155 - acc: 0.9952 - val_loss: 0.0362 - val_acc: 0.9885\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 661us/step - loss: 0.0146 - acc: 0.9959 - val_loss: 0.0361 - val_acc: 0.9880\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 666us/step - loss: 0.0137 - acc: 0.9960 - val_loss: 0.0356 - val_acc: 0.9883\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 664us/step - loss: 0.0143 - acc: 0.9959 - val_loss: 0.0364 - val_acc: 0.9883\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 665us/step - loss: 0.0133 - acc: 0.9961 - val_loss: 0.0362 - val_acc: 0.9885\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 662us/step - loss: 0.0139 - acc: 0.9956 - val_loss: 0.0361 - val_acc: 0.9878\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 666us/step - loss: 0.0131 - acc: 0.9961 - val_loss: 0.0371 - val_acc: 0.9878\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 661us/step - loss: 0.0130 - acc: 0.9966 - val_loss: 0.0369 - val_acc: 0.9880\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 667us/step - loss: 0.0128 - acc: 0.9961 - val_loss: 0.0370 - val_acc: 0.9883\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 669us/step - loss: 0.0128 - acc: 0.9966 - val_loss: 0.0373 - val_acc: 0.9878\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 664us/step - loss: 0.0128 - acc: 0.9962 - val_loss: 0.0377 - val_acc: 0.9878\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 665us/step - loss: 0.0122 - acc: 0.9965 - val_loss: 0.0381 - val_acc: 0.9878\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 657us/step - loss: 0.0118 - acc: 0.9967 - val_loss: 0.0372 - val_acc: 0.9878\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 660us/step - loss: 0.0119 - acc: 0.9965 - val_loss: 0.0367 - val_acc: 0.9884\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 660us/step - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0374 - val_acc: 0.9885\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 661us/step - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0373 - val_acc: 0.9880\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 660us/step - loss: 0.0111 - acc: 0.9967 - val_loss: 0.0366 - val_acc: 0.9887\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 662us/step - loss: 0.0109 - acc: 0.9966 - val_loss: 0.0370 - val_acc: 0.9881\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 669us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0378 - val_acc: 0.9887\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 657us/step - loss: 0.0106 - acc: 0.9968 - val_loss: 0.0380 - val_acc: 0.9881\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 662us/step - loss: 0.0101 - acc: 0.9973 - val_loss: 0.0380 - val_acc: 0.9885\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 665us/step - loss: 0.0112 - acc: 0.9964 - val_loss: 0.0371 - val_acc: 0.9879\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 663us/step - loss: 0.0101 - acc: 0.9971 - val_loss: 0.0395 - val_acc: 0.9873\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 662us/step - loss: 0.0102 - acc: 0.9972 - val_loss: 0.0376 - val_acc: 0.9881\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 660us/step - loss: 0.0099 - acc: 0.9971 - val_loss: 0.0377 - val_acc: 0.9879\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 663us/step - loss: 0.0102 - acc: 0.9969 - val_loss: 0.0371 - val_acc: 0.9886\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 662us/step - loss: 0.0101 - acc: 0.9971 - val_loss: 0.0393 - val_acc: 0.9869\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 666us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0393 - val_acc: 0.9875\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 665us/step - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0391 - val_acc: 0.9875\n",
      "Train on 1049 samples, validate on 450 samples\n",
      "Epoch 1/2\n",
      "1049/1049 [==============================] - 1s 666us/step - loss: 0.0102 - acc: 0.9968 - val_loss: 0.0374 - val_acc: 0.9877\n",
      "Epoch 2/2\n",
      "1049/1049 [==============================] - 1s 659us/step - loss: 0.0096 - acc: 0.9972 - val_loss: 0.0369 - val_acc: 0.9881\n"
     ]
    }
   ],
   "source": [
    "for ii in range(20):\n",
    "    hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=2, validation_data=(X_val, y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "_cell_guid": "5c703574-cf76-495f-b800-1fc6c9384ded",
    "_uuid": "ddc5afff4b22841fb184e32cc4b19a2225dec451",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.98159509202453"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(YY_test==y_pred.argmax(axis=1))/len(acc)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
